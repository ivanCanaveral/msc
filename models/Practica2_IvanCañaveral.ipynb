{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19msGdkGTRmF"
   },
   "source": [
    "# Páctica 2: Redes convolucionales\n",
    "\n",
    "**Alumno**: Iván Cañaveral Sánchez\n",
    "\n",
    "En esta práctica vamos a explorar e intentar comprender el funcionamiento de las redes convolutivas para el procesamiento de textos.\n",
    "\n",
    "La práctica se estructura en las siguientes partes:\n",
    "\n",
    "* **1. Comparativa con modelos revisados en la práctica anterior**, así como las técnicas de vectorización asociadas.\n",
    "* **2. Exploración detallada** de este tipo de modelos, explorando cómo funcionan los filtros y cómo se generan los mapas de características, antes de comenzar a hacer pruebas con hiperparámetros.\n",
    "* **3.** Finalmente, una vez que tenemos un entendimiento básico del funcionamiento interno estos modelos, llevaremos a cabo una serie de pruebas para ver cómo **impacta la modificación de ciertos hiperparámetros**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv-V8nKFVYyA"
   },
   "source": [
    "## 0. Librerías y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHHgcdbcDbMf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import string\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agcNCkVyFtah"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4Bx6OZPR05V"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9_1LVqYG9IC"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgaV3E8ydX52"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZePYlXIHQQI",
    "outputId": "8d13444e-7d97-4555-fcf9-f5dc86ecb710"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kj1S1Ol8FLmp"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaSQ88AJVJ2-"
   },
   "source": [
    "## 1. Comparativa con modelos anteriores. Vectorización.\n",
    "\n",
    "En esta sección vamos a comparar el funcionamiento de una red convolutiva simple con un perceptrón multicapa, prestando atención a los distintos métodos de vectorización.\n",
    "\n",
    "Una diferencia importante en este punto es que los modelos vistos previamente requerían una representación que generase un único vector por cada texto o documento. De hecho, en la práctica anterior, cuando se utilizaban vectorizaciones a nivel de palabra, forzábamos la representación a nivel de texto utilizando una capa \"flatten\" en el modelo (también podrían haberse usado operaciones como medias, cálculo de centros, etc.)\n",
    "\n",
    "En los modelos convolucinales, cada textos debe ser representado como una secuencia ordenada de palabras, para que estos modelos puedan detectar patrones dentro de las secuencias de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi7Qiwo_Y0uU"
   },
   "source": [
    "### Carga y limpieza del texto\n",
    "\n",
    "En este apartado vamos a descargar, limpiar y cargar los textos en un DataFrame de pandas, que dado que el tamaño no es muy grande, mantendremos cargado en memoria.\n",
    "\n",
    "Dado que el proceso es el mismo que ne la práctica anterior, no entraremos en grandes detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAkWeHap-hXu",
    "outputId": "e88e35cf-bd49-4b35-c6d1-49f7223ff71a"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ivanCanaveral/msc-datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE0WOPxaAXz7"
   },
   "outputs": [],
   "source": [
    "raw_test_data = pd.read_csv('msc-datasets/movie-reviews/test_reviews.csv')\n",
    "raw_train_data = pd.read_csv('msc-datasets/movie-reviews/train_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_q676CvDZFJ"
   },
   "outputs": [],
   "source": [
    "raw_test_data['partition'] = 'test'\n",
    "raw_train_data['partition'] = 'train'\n",
    "dataset = pd.concat([raw_test_data, raw_train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "Deen6AHfDmYh",
    "outputId": "d66df35b-a72b-4d1d-835e-297e240658a1"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.set_index(dataset.id).drop(columns=[\"id\"])\n",
    "dataset[\"length\"] = dataset.review.str.split().apply(len)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kKYcnk2ZfZH"
   },
   "source": [
    "Revisamos rápidamente cuántos textos tenemos en cada una de las particiones y clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kQIbutp8D2d5",
    "outputId": "9908ccd2-2093-4e9c-85a1-48033f38421e"
   },
   "outputs": [],
   "source": [
    "dataset.groupby(by=[\"partition\", \"sentiment\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u8Bg3lzZpyD"
   },
   "source": [
    "A continuación tenemos la clase que utilizamos para limpiar y procesar los textos. Ofrece 3 niveles distintos de procesado:\n",
    "\n",
    "* **basic**: limpieza básica de stopwords, paso a minúsculas, etc.\n",
    "* **lemma**: aplica lematización\n",
    "* **stem**: aplica stemming\n",
    "\n",
    "Los detalles de cada una de ellas se revisaron en la práctica anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Lv_0i71Efny"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, level='basic'):\n",
    "        assert level in ['basic', 'lemma', 'stem'], \"Wrong level value\"\n",
    "        self.level = level\n",
    "        self.apply_lemma = level != 'basic'\n",
    "        self.apply_stem = level == 'stem'\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "        words = letters_only.lower().split()\n",
    "        stops = set(stopwords.words(\"english\") + ['br'])\n",
    "        words = [w for w in words if not w in stops]\n",
    "        return words\n",
    "\n",
    "    def lemmatize_words(self, words):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "        return lemmatized\n",
    "    \n",
    "    def stem_words(self, words):\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed = [stemmer.stem(word) for word in words]\n",
    "        return stemmed\n",
    "\n",
    "    def parse_text(self, text):\n",
    "        words = self.clean_text(text)\n",
    "        if self.apply_lemma:\n",
    "            words = self.lemmatize_words(words)\n",
    "        if self.apply_stem:\n",
    "            words = self.stem_words(words)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.vectorize(self.parse_text)(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"level\": self.level\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tz1Fivhvay1G"
   },
   "source": [
    "Vamos a aplicar los tres tipos de preprocesado para poner disponer de ellos de cara a llevar a cabo distintas pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvmlBWRYFrnl",
    "outputId": "c5368eca-6809-4177-e44f-ca417fe59b3d"
   },
   "outputs": [],
   "source": [
    "for prep_type in ['basic', 'lemma', 'stem']:\n",
    "  print(\"Procesando...\", prep_type, end=' ')\n",
    "  t0 = time.time()\n",
    "  text_preprocessor = TextPreprocessor(prep_type)\n",
    "  dataset[f'{prep_type}_review'] = text_preprocessor.transform(dataset.review)\n",
    "  print(f\"{time.time()-t0:02f} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2I_KDiqcA-I"
   },
   "source": [
    "Generamos una etiqueta binaria para la variable \"sentiment\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCAy6_SpGPe-"
   },
   "outputs": [],
   "source": [
    "dataset['label'] = dataset['sentiment'] == 'positive'\n",
    "dataset['label'] = dataset['label'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUpqbaz-cPj-"
   },
   "source": [
    "A continuación mostramos la estructura del dataset generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "FqgYL5eCOhOJ",
    "outputId": "c9858d7a-dc37-4d24-8203-5d2edc6b8579"
   },
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zV4xxfWdSKy"
   },
   "source": [
    "Para facilitar los desarrollos posteriores vamos a generar dos DataFrames separando el conjunto de datos de entrenamiento y el de evaluación (train y test respectivamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PCwWScYFfEq"
   },
   "outputs": [],
   "source": [
    "train_df = dataset[dataset['partition'] == 'train']\n",
    "test_df = dataset[dataset['partition'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgrJ174QclsM"
   },
   "source": [
    "### Vectorización\n",
    "\n",
    "Por simplicidad, en adelante vamos a desarrollar los modelos en la librería `tensorflow`en la medida de lo posible, por lo que para facilitar los entrenamientos vamos a cargar los datos en un Dataset de tensorflow, que dispone de algunas ventajas frente al dataset de pandas que se había utilizado hasta ahora, tales como uno control sencillo de los batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x62ROHcsGls3"
   },
   "outputs": [],
   "source": [
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['lemma_review'], train_df['label'].values)).batch(BATCH_SIZE)\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df['lemma_review'], test_df['label'].values)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQHrJ6d_fP6v"
   },
   "source": [
    "Exploramos cómo quedan los textos y las etiquetas dentro de estos datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddrlfMruGl2o",
    "outputId": "47820867-a3c3-4eb8-8782-20aacb9708d7"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(\"Review: \", text_batch.numpy()[i][:200])\n",
    "    print(\"Label:\", label_batch.numpy()[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEMS5AdoeVjQ"
   },
   "source": [
    "Hasta este momento los pasos seguidos para la generación del dataset han sido comunes. A partir de ahora, durante este apartado, realizaremos una comparativa entre paso a paso.\n",
    "\n",
    "De cara al **perceptrón**, dado que ya se exploraron las distintas posibilidades de vectorización, así que para este ejercicio vamos a elegir únicamente una de ellas. Vamos a elegir una **vectorización binaria**, donde para cada review, tendremos un vector de tamaño `VOCAB_SIZE`, que nos indicará si cada una de las palabras del vocabulario estaban presentes o no en el texto. Nos referiremos a las variables respectivas a este modelo con el prefijo `binary_*`.\n",
    "\n",
    "Respecto al **modelo convolucional**, transformaremos el texto en una secuencia de índices de palabras, con el propósito de incluir un **embedding** a continuación. Se podría llegar a entrenar un modleo directamente utilizando los índices (sin utilizar embedding), pero dado que el índice del vocabulario no guarda ingún tipo de relación con el contenido semántico de las palabras (únicamente se tiene en cuenta su frecuencia), los resultados serían previsiblemente peores dado el ruido introducido. En ambos procesos utilizaremos el mismo tamaño de vocabulario `VOCAB_SIZE`. Nos referiremos a las variables respectivas a este modelo con el prefijo `seq_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYtmBksWHhCf"
   },
   "outputs": [],
   "source": [
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Bhd0q_0IEXR"
   },
   "outputs": [],
   "source": [
    "seq_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShUv1nBajICv"
   },
   "source": [
    "Generamos un dataset sin etiquetas para ajustar los vectorizadores, y los ajustamos (selección de vocabularios, generación de índices, etc).\n",
    "\n",
    "**Nota:** Estos vectorizadores podían incluirse dentro de los modelos, pero por el momento los mantendremos fuera, para facilitar la exploración de los modelos y vectorizaciones, y también por términos de eficiencia computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5arPj0tYIFfd"
   },
   "outputs": [],
   "source": [
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "seq_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmwRRHm1jf3n"
   },
   "source": [
    "Creamos dos funciones para aplicar la vectorización en los datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RunHRGEpIFis"
   },
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjhsNuemIFo5"
   },
   "outputs": [],
   "source": [
    "def seq_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return seq_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weFa2LWNliyX"
   },
   "source": [
    "A continuación comparamos los resultados de ambos procesos de vectorización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPgOU6vGIFsm",
    "outputId": "921c475a-b54e-437b-fd74-67680ee0955b"
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review: \", first_review.numpy()[:200])\n",
    "print(\"Label: \", first_label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNjjD8rQmDCC"
   },
   "source": [
    "#### Ejemplo de vectorización binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NfIMA99IQGs",
    "outputId": "860a7069-194a-421f-d8bd-bc4430f28bdf"
   },
   "outputs": [],
   "source": [
    "print(\"Vectorización binaria:\")\n",
    "print(binary_vectorize_text(first_review, first_label)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ4zkLz9oULD"
   },
   "source": [
    "#### Ejemplo de vectorización secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLAFjlvsIQJw",
    "outputId": "1e690d5b-b773-419f-99e0-a47b305d677a"
   },
   "outputs": [],
   "source": [
    "print(\"Vectorización secuencial:\")\n",
    "print(seq_vectorize_text(first_review, first_label)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhBjy43InFc9"
   },
   "source": [
    "En el caso de la vectorización secuencial, podemos recuperar el texto original (salvo palabras fuera del vocabulario), dado que seguimos manteniendo el orden del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJgrPKnaIQML",
    "outputId": "e3cf513d-2606-4e75-9c4d-83eba4fe7b9a"
   },
   "outputs": [],
   "source": [
    "print(\"2 ---> \", seq_vectorize_layer.get_vocabulary()[2])\n",
    "print(\"3 ---> \", seq_vectorize_layer.get_vocabulary()[3])\n",
    "print(\"Vocabulary size: {}\".format(len(seq_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOpAvD6TneX-",
    "outputId": "e1df80cf-1176-4591-fe86-8fb6aac4e7e0"
   },
   "outputs": [],
   "source": [
    "print(\"Reverse vectorization:\")\n",
    "for word_index in seq_vectorize_text(first_review, first_label)[0].numpy()[0]:\n",
    "  print(seq_vectorize_layer.get_vocabulary()[word_index], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2505EUtoRio"
   },
   "source": [
    "### Preparación de datasets\n",
    "\n",
    "En las siguientes celdas de código vamos a, finalmente incluir las vectorizazaciones en los datasets, y a hacer ajustes para paralelizar la vectorización en equipos con varios cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nc2qhuRZIQPa"
   },
   "outputs": [],
   "source": [
    "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
    "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
    "\n",
    "seq_train_ds = raw_train_ds.map(seq_vectorize_text)\n",
    "seq_test_ds = raw_test_ds.map(seq_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgaRI7EzIQSv"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJHy19NrIQVa"
   },
   "outputs": [],
   "source": [
    "binary_train_ds = configure_dataset(binary_train_ds)\n",
    "binary_test_ds = configure_dataset(binary_test_ds)\n",
    "\n",
    "seq_train_ds = configure_dataset(seq_train_ds)\n",
    "seq_test_ds = configure_dataset(seq_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii158mqppp3W"
   },
   "source": [
    "### Modelos\n",
    "\n",
    "A continuación generaremos, entrenaremos y evaluaremos ambos modelos.\n",
    "\n",
    "**Nota**: A la hora de modelizar el problema, por las características del mismo, podríamos hacerlo utilizando modelos con un output bidimensional (una para la categoría \"positive\" y otro para la categoría negative), o un único output unidimensional que nos indique si es positivo o negativo. En nuestro caso, una opción un otra no debería representar una diferencia grande en términos de precisión o explicabilidad de los modelos. Las únicas dos consideraciones en este punto serían:\n",
    "* Un output binario implicaría un **mayor número de parametros** a ajustar. Es posible que internamente se replicasen pesos con polaridades opuestas.\n",
    "* La función de pérdida elegida sería distinta. `SparseCategoricalCrossentropy` o `CategoricalCrossentropy` cuando se enfoca com un problema multiclase y `BinaryCrossentropy` cuando se enfoca como un problema de regresión. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHbxKa2W5qPh"
   },
   "source": [
    "#### Red simple (perceptron monocapa)\n",
    "\n",
    "Vamos a crear u perceptrón monocapa para resolver un problema de dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW0JlKFI12zI"
   },
   "outputs": [],
   "source": [
    "def create_simple_network():\n",
    "  model = tf.keras.Sequential([layers.Dense(2)])\n",
    "  model.compile(\n",
    "      loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer='adam',\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxNw2qP72fjW",
    "outputId": "7f6988b9-7ed3-4b4d-f12c-d326102d0f4c"
   },
   "outputs": [],
   "source": [
    "simple_network = create_simple_network()\n",
    "history = simple_network.fit(\n",
    "    binary_train_ds, validation_data=binary_test_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubNIwN9A7fmt",
    "outputId": "216d616f-1a64-4487-fbef-b8d636a8475d"
   },
   "outputs": [],
   "source": [
    "simple_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmNDUtZb6AzS"
   },
   "source": [
    "#### Red convolutiva\n",
    "\n",
    "Vamos a entrenar una red convolutiva simple con un una única capa convolutiva, de 64 filtros.\n",
    "\n",
    "El objetivo de esta red es simplemte comparar los primeros resultados. Después entreremos más en detalle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ueWUT6BIQa7"
   },
   "outputs": [],
   "source": [
    "def create_simple_conv_model(vocab_size, num_labels):\n",
    "  if num_labels == 1:\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "  else:\n",
    "    loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  \n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  model.compile(\n",
    "    loss=loss,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmhNzY_F3nL4",
    "outputId": "6d297d6c-f16b-415e-fb8a-90426969a2d2"
   },
   "outputs": [],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "conv_model = create_simple_conv_model(vocab_size=VOCAB_SIZE + 1, num_labels=2)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWnZzaJh6qvO",
    "outputId": "b836f72d-29bd-49f1-c98e-c545332861e9"
   },
   "outputs": [],
   "source": [
    "history = conv_model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO5BMEaB6vLU"
   },
   "source": [
    "Vamos a repetir este proceso, pero para un modelo con una única clasae, para confimar que no hay grandes diferencias en cuanto a la precisión del mismo, como anticipábamos previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUtsZ7fgJfUL",
    "outputId": "5b3a1b7a-10ae-495b-9cf9-8055eabe1401"
   },
   "outputs": [],
   "source": [
    "conv_model = create_simple_conv_model(vocab_size=VOCAB_SIZE + 1, num_labels=1)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPovIeh17Gp-"
   },
   "source": [
    "Como vemos, el número de parámetros de la última capa en este caso queda reducido a la mitad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbVCmQWcK8sB",
    "outputId": "7438697a-e3e9-400c-9067-d579f4b6d1cf"
   },
   "outputs": [],
   "source": [
    "history = conv_model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgomzVAs837H"
   },
   "source": [
    "Podemos observar que entre las dos versiones de los modelos convolucionales que hemos entrenado no hay apenas diferencias (ambos moelos se quedan en un 83% de precisión). Sin embargo, la fiferencia es notable a la hora de comparar con un modelo más sencillo, como es el perceptrón monocapa (87% de precisión).\n",
    "\n",
    "Dejando a un lado variables como el tamaño del dataset número de epochs, se aprecia que los modelos convolucionales muestran un claro overfitting:\n",
    "* perceptrón multicapa: `train acc 96% -  test acc 87%`\n",
    "* rec convolucional: `train acc 100% - test acc 83%`\n",
    "\n",
    "Algo que encaja a la perfección con el hecho de que el segundo modelo tenga un número considerablemente mayor de parámetros (unas 30 veces mayor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z--a4F24AR4x"
   },
   "source": [
    "### Exploración\n",
    "\n",
    "Vamos a realizar una exploración rápida de ambos modelos, con el fin de establecer un pequeño paralelismo que sirva como introducción al siguiente apartado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tco0XJB3DEy-"
   },
   "source": [
    "Si atendemos a los pesos que los modelos usan para decidir la categoría del texto, en el primer caso tenemos los pesos asociados directamente a la variable binaria que indica si una palabra está presente en el texto o no.\n",
    "\n",
    "Si atendemos a las palabras cuyas apariciones son más relevantes para la clasificación, es relatiamente sencillo entender el las decisiones que toma el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idEMVeemEOSr"
   },
   "source": [
    "Palabras con mayor peso para la categoría negativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R_dN8MrNyhp"
   },
   "outputs": [],
   "source": [
    "simple_network_weights = {k:v for k,v in zip(\n",
    "    binary_vectorize_layer.get_vocabulary(), simple_network.layers[0].get_weights()[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEOZjUsoOUGA",
    "outputId": "a2887097-129e-4149-b549-01ccf5577833"
   },
   "outputs": [],
   "source": [
    "sorted_simple_network_weights = {k: v for k, v in sorted(\n",
    "    simple_network_weights.items(), key=lambda item: item[1][0], reverse=True)}\n",
    "for k, v in list(sorted_simple_network_weights.items())[:10]:\n",
    "  print(k, v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oTbxHSuIqXb"
   },
   "source": [
    "Palabras con mayor peso para la categoría positiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txAkXKAmOuZW",
    "outputId": "59fa1bfb-4df2-4edc-b66e-af6c4009db80"
   },
   "outputs": [],
   "source": [
    "sorted_simple_network_weights = {k: v for k, v in sorted(\n",
    "    simple_network_weights.items(), key=lambda item: item[1][1], reverse=True)}\n",
    "for k, v in list(sorted_simple_network_weights.items())[:10]:\n",
    "  print(k, v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uhms3If0JIPe"
   },
   "source": [
    "Si atendemos ahora al modelo convolutivo, vemos que las decisiones se toman en base a la activación o no de 64 filtros a lo largo del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7zrDVITKhZu",
    "outputId": "21292e60-8be8-4907-9722-e0c54cc30514"
   },
   "outputs": [],
   "source": [
    "conv_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lFJF2goJoyR",
    "outputId": "b820a72c-b99b-4ddd-c867-07be85008774"
   },
   "outputs": [],
   "source": [
    "conv_model.layers[-1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJDj5vx1JK8w"
   },
   "source": [
    "Entender la capa densa del clasificador es inmediato, una vez que se entiende el funcionamiento de los filtros previos, y los mapas de caracteríasticas que generan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUM3RQ8_L8KG"
   },
   "source": [
    "## Exploración detallada.\n",
    "\n",
    "En esta sección vamos a profundizar en la clave de los modelos convolutivos: los filtros y los mapas de características, que no son más que mapas vectoriales la distribución de la activación de los filtros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jRWrE1RMnq2"
   },
   "source": [
    "Como primer paso, vamos a generar un promer modelo que nos sirva de guía para la exploración del mismo. Dado que los modelos anteriores mostraban cierto overfitting, introduciremos capas de Dropuot como primera medida para reducirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hh4fkFoOrsg"
   },
   "source": [
    "### Definición del primer modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sh8vY2NPPdE2"
   },
   "outputs": [],
   "source": [
    "## Model Graph\n",
    "# A integer input for vocab indices.\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "# 'embedding_dim'.\n",
    "emb = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "emb_drop = layers.Dropout(0.5)(emb)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "fmaps = layers.Conv1D(32, 5, padding=\"valid\", activation=\"relu\", strides=1)(emb_drop)\n",
    "pfmaps = layers.GlobalMaxPooling1D()(fmaps)\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "classifier = layers.Dense(32, activation=\"relu\")(pfmaps)\n",
    "classifier_drop = layers.Dropout(0.5)(classifier)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(classifier_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HouvG-PmNeWO"
   },
   "outputs": [],
   "source": [
    "## Model build\n",
    "conv_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "conv_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYVXhPzqN5rg",
    "outputId": "831cbdc0-f170-4ca6-edcc-d662e222ed3e"
   },
   "outputs": [],
   "source": [
    "conv_model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1j59qrPPApf"
   },
   "source": [
    "### Modelo de activación\n",
    "\n",
    "Partiendo del modelo que acabamos de entrenar, vamos a generar un modelo cuyo output sean los mapas de activación de los distintos filtros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCkcAcdAP5Nm"
   },
   "outputs": [],
   "source": [
    "def gen_activation_model(model):\n",
    "  layer_outputs = []\n",
    "  layer_names = []\n",
    "  for layer in conv_model.layers:\n",
    "      if isinstance(layer, (tf.keras.layers.Conv1D, tf.keras.layers.GlobalMaxPooling1D)):\n",
    "          layer_outputs.append(layer.output)\n",
    "          layer_names.append(layer.name)\n",
    "  activation_model = tf.keras.Model(inputs=model.inputs, outputs=layer_outputs)\n",
    "  return layer_names, activation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE8iAzRxQWcR"
   },
   "outputs": [],
   "source": [
    "layer_names, activation_model = gen_activation_model(conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jhn993WQm1U"
   },
   "source": [
    "Vamos también a crear una frase de ejemplo que tenga patrones positivos y negativos en una misma frase, que nos sirva para estudiar los distintos patrones de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWiFN7GgRntJ"
   },
   "outputs": [],
   "source": [
    "good_bad_review = \"Last Thursday I went to the movies to see Tarantino's latest film. \\\n",
    "    I thought the beginning of the movie was horrible, \\\n",
    "    and a complete waste of time. Boring, the plot was poor and disappointing. \\\n",
    "    However, everything changed after the first 15 minutes. The rest of the movie was \\\n",
    "    perfect, with a brilliant plot and great performances.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqOoZNXJXM8E"
   },
   "outputs": [],
   "source": [
    "t = TextPreprocessor('lemma')\n",
    "good_bad_review = t.transform([good_bad_review])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lkn_LmbJRpYU",
    "outputId": "336a4bd6-94bd-45ae-c4a9-120edd21cbba"
   },
   "outputs": [],
   "source": [
    "sample_vecs = seq_vectorize_layer([good_bad_review])\n",
    "feature_maps, pooled_feature_maps = activation_model.predict(sample_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJiPwR1sSiNd"
   },
   "source": [
    "Dado que nuestro modelo tenía 32 filtros, y una longitud máxima de 250 tokens, para un texto tendremos los 32 mapas de activación crrespondientes, consistentes en 246 evaluaciones (debido al tamaño del filtro, strides y padding elegido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoJE4_PxSb94",
    "outputId": "12c8b3ad-121b-4626-bc01-e447a118b6c1"
   },
   "outputs": [],
   "source": [
    "feature_maps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNfWpc8LTmlV"
   },
   "source": [
    "Posteriormente, se añade una capa GlobalMaxPooling1D, que por cada mapa de activación, nos devolverá su valor máximos. Básicamente esto nos indicará si se ha detectado un patrón concreto en el texto y su nivel de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGu3XXS9SGqq",
    "outputId": "f1e0d274-7390-45c4-d1b0-bc4d11c69540"
   },
   "outputs": [],
   "source": [
    "pooled_feature_maps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CacYGXhbUZt1"
   },
   "source": [
    "### Activación de un filtro específico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSa8osVaUiNA"
   },
   "source": [
    "De este modo, disponemos de la vectorización del texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Vyaq27eUmYX",
    "outputId": "a9758d7a-617d-400f-8feb-ced9562f1cd3"
   },
   "outputs": [],
   "source": [
    "sample_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BinKaztpVPJo"
   },
   "source": [
    "Para el décimo filtro, por ejemplo, el modelo tiene una activación de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1_dxdffSGap",
    "outputId": "6de85d71-b6f9-4152-ac17-74b152f3ae73"
   },
   "outputs": [],
   "source": [
    "pooled_feature_maps[0][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7wtjKuSVY0J"
   },
   "source": [
    "Cuyo mápa de características ha sido, para este texo:\n",
    "\n",
    "*únicamente mostramos la evaluación de las primeras 50 ventanas de activación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfuE1sgORpSD",
    "outputId": "23101f4e-8eea-4302-a120-114cf959fd13"
   },
   "outputs": [],
   "source": [
    "feature_maps[0,:,10][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL09SUbFitkg"
   },
   "outputs": [],
   "source": [
    "vocab = seq_vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7nT7RN7V73B"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for word_index in sample_vecs[0].numpy():\n",
    "  if word_index > 0:\n",
    "    words.append(seq_vectorize_layer.get_vocabulary()[word_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH20G4CGWsu7"
   },
   "outputs": [],
   "source": [
    "activations = []\n",
    "for i in range(2, len(words)-2):\n",
    "  window = ' '.join([words[i + j] for j in range(-2,3)])\n",
    "  activations.append({'index': i, 'window':window, 'activation': feature_maps[0,i,10]})\n",
    "activations = pd.DataFrame(activations).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "Xti09STnWsRv",
    "outputId": "b14c7dc7-02b4-44ab-a13d-e59797ce159a"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=activations, x='window', y='activation', linewidth=2.5)\n",
    "_ = plt.xticks(rotation = 'vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJqEo11Qdoum"
   },
   "source": [
    "Podemos ver cómo la activación tras aplicar este filtro es bastante alta cuando apaarecen expresiones positivas, mientras que cuando no es así permanece baja. De hecho, este filtro parece encargarse de detectar patrones principalmente positivos, dado que no hay activación con palabras claramente negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mo3jAxkfkTA"
   },
   "source": [
    "### Patrones de activación\n",
    "\n",
    "Ahora que hemos repasado en detalle una pequeña parte del modelo, vamos a intentar obtener una visión más global del mismo.\n",
    "\n",
    "Para ello, en primer lugar vamos a intentar visualizar la activación de odos los filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv0a4i_tgs3A"
   },
   "outputs": [],
   "source": [
    "def process_feature_maps(sample_vecs, feature_maps, vocab, n_filters=32):\n",
    "  feature_maps_data = []\n",
    "  for fmap_index in range(n_filters):\n",
    "    #print('fmap', fmap_index)\n",
    "    for i, token_index in enumerate(sample_vecs[0]):\n",
    "      if token_index > 1:\n",
    "        try: # cambiar esto para que itere sobre los fmaps\n",
    "          #print(i, vocab[token_index.numpy()], sample_vecs[0][i].numpy(), feature_maps[0,i,fmap_index])\n",
    "          feature_map_data = {}\n",
    "          feature_map_data['filter'] = fmap_index\n",
    "          feature_map_data['word'] = vocab[token_index.numpy()]\n",
    "          feature_map_data['word_index'] = sample_vecs[0][i].numpy()\n",
    "          feature_map_data['word_filter_weight'] = feature_maps[0,i,fmap_index]\n",
    "          feature_maps_data.append(feature_map_data)\n",
    "        except IndexError:\n",
    "          pass\n",
    "    print()\n",
    "  feature_maps_data = pd.DataFrame(feature_maps_data)\n",
    "  return feature_maps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuNE4gAAiBcn",
    "outputId": "adf603c9-b05c-48c9-d0aa-ad3799562c76"
   },
   "outputs": [],
   "source": [
    "feature_maps_data = process_feature_maps(sample_vecs, feature_maps, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tYSVOB-oZfh"
   },
   "source": [
    "**Nota**: Para facilitar la visualización y dado que nuestra ventana es de 5 tokens, en el fráfico de activación vamo s amostrar únicamente la palabra palabra central de cada ventana en el ejehorizontal, para evitar grandes líneas de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "L7xSIKLxnSMQ",
    "outputId": "e589a59a-b1ab-4499-b13f-d5750035a4cb"
   },
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    data=feature_maps_data,\n",
    "    x='word',\n",
    "    y='word_filter_weight',\n",
    "    hue='filter',\n",
    "    linewidth=2.5\n",
    ")\n",
    "_ = plt.xticks(rotation = 'vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "703wfFploC8w"
   },
   "source": [
    "Como se puede observar, en una vista global del modelo, hay grandes activaciones cuando se dan expresiones claramente positivas y/o negativas.\n",
    "\n",
    "Podemos observar que muchos filtros se activan en las mismas zonas, lo que podría indicar que el modelo está aprendiendo patrones redundantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4YL3mx4wuZS"
   },
   "source": [
    "#### Exploración de mapas de características\n",
    "\n",
    "También podemo sobservar cómo hay filtros que no se activan o lo hanpoco . Para tener un mejor detalle de esto último, vamos a intentar generar histogramas de cada uno de los mapas de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr4nLePCpVAE"
   },
   "outputs": [],
   "source": [
    "feature_maps_data['column'] = feature_maps_data['filter'] // 10\n",
    "feature_maps_data['row'] = feature_maps_data['filter'] % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "66mewFrZpWGf",
    "outputId": "7536decd-da6a-4d68-844d-c858ae3d60ff"
   },
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    feature_maps_data,\n",
    "    x=\"word_filter_weight\",\n",
    "    col=\"column\",\n",
    "    row=\"row\",\n",
    "    binwidth=0.05,\n",
    "    binrange=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQfDDumnp78l"
   },
   "source": [
    "Podemos ver que hay histogramas que reflejan muchos valores cercanos a cero. No es necesariamente un problemadado que únicamente estamos viendo una frase concreta, y no un dataset completo, pero podría ser un indicador de que el modelo está manejando muchos más parámetros de los necesarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1kp91Eazaqp"
   },
   "source": [
    "A través de una exploración manual, sí podemos encontrar algunos filtros que aprenden a detectar patrones irrelevantes para el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "TknHGMg9zDvG",
    "outputId": "24d51094-eccd-462c-8b39-e0710c772312"
   },
   "outputs": [],
   "source": [
    "feature_maps_data[feature_maps_data['filter'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKweeQ8qxKW-"
   },
   "source": [
    "#### Aprendizaje de filtros\n",
    "\n",
    "Vamos a revisar algunos histogramas de los pesos de los filtros de la capa convolutiva, para detectar si existen filtros que no están aprendiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVrmguU6pWBh",
    "outputId": "b92cc8c0-4861-4446-969d-1dded6f4892e"
   },
   "outputs": [],
   "source": [
    "conv_model.layers[3].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4BC6lVys3ql"
   },
   "outputs": [],
   "source": [
    "filter_data = []\n",
    "filters = conv_model.layers[3].get_weights()[0]\n",
    "for token in range(5):\n",
    "  for dim in range(64):\n",
    "    for i, weight in enumerate(filters[token][dim]):\n",
    "      filter_data.append(\n",
    "          {\n",
    "              'token': token,\n",
    "              'dim': dim,\n",
    "              'filter': i,\n",
    "              'weight': weight\n",
    "          }\n",
    "      )\n",
    "filter_data = pd.DataFrame(filter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VGEVmKpRvGRw",
    "outputId": "6861a616-11a7-4170-e6b1-ca0bc38903d4"
   },
   "outputs": [],
   "source": [
    "filter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb8BirRDu4L7"
   },
   "outputs": [],
   "source": [
    "filter_data['column'] = filter_data['filter'] // 10\n",
    "filter_data['row'] = filter_data['filter'] % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xS-9zZ1svBfc",
    "outputId": "777390ba-b250-42a5-c72a-4fa102446eb9"
   },
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    filter_data,\n",
    "    x=\"weight\",\n",
    "    col=\"column\",\n",
    "    row=\"row\",\n",
    "    binwidth=0.05,\n",
    "    binrange=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FjNGPAZxs_U"
   },
   "source": [
    "Podemos observar que en este punto no existen filtros cuyos pesos sean todos muy cercanos a cero. Este problema suele ser común en modelos que trabajan con imágenes, donde se concatenan varias capas convolucionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsE852vRyD9v"
   },
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGIgped0ztCq"
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "Un componenete fundamental en este model es el embbeding que se utiliza para vectorizar los textos. Vamos a hacer una primera exploración para comprobar que tiene sentido.\n",
    "\n",
    "Dado que intentar explorar todo el vocabulario es un porblema complejo  que daría para una práctica en sí misma, vamos a intentar centrarnos en el ejemplo con el que venimos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gDX-2Q9bXvJ"
   },
   "outputs": [],
   "source": [
    "seq_vocab = seq_vectorize_layer.get_vocabulary()\n",
    "word_embeddings = conv_model.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGmy7jKW1QMR"
   },
   "source": [
    "Para facilitar la visualización de los embeddings, vamos a utilizar TSNE para reducir la dimensionalidad. Adicionalmente, mostraremos únicamente las palabras del texto de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q81HrgrVA8ct"
   },
   "outputs": [],
   "source": [
    "word_embeddings_2 = TSNE(\n",
    "    n_components=2,\n",
    "    learning_rate='auto',\n",
    "    init='random',\n",
    "    perplexity=30).fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nm867aSgA8R8",
    "outputId": "0e70d554-7aec-4700-9739-4b7c493b51a4"
   },
   "outputs": [],
   "source": [
    "word_embeddings_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZPXJGFLDiQS"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(word_embeddings_2, columns=['x', 'y'])\n",
    "# adding a columns for the corresponding words\n",
    "df['words'] = seq_vocab\n",
    "df['in_sentence'] = df['words'].isin(feature_maps_data['word'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoA0GF5dD1cn"
   },
   "outputs": [],
   "source": [
    "df = df[df['in_sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "IVHK_jSuBMaA",
    "outputId": "3274a774-746b-40df-ebb3-ebf48ecf4899"
   },
   "outputs": [],
   "source": [
    "# plotting a scatter plot\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", text=\"words\")\n",
    "# adjusting the text position\n",
    "fig.update_traces(textposition='top center')\n",
    "# setting up the height and title\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text='Word embedding chart'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9g8k3gJ2yLh"
   },
   "source": [
    "Podemos observar como hay regiones donde se sitúan todas las palabras negativas, y otras donde se acumulan las positivas, que es el comportamiento esperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkvxlFXa5VUI"
   },
   "source": [
    "## Pruebas\n",
    "\n",
    "Ahora que ya hemos entendido en detalle cómo funcionnan los modelos convolucionales aplicados a texto, vamos a llevar a cabo una serie de pruebas que nos ayuden a entender mejor el impacto de las distintas configuraciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DugsPDqY5vub"
   },
   "source": [
    "### Generación de modelos\n",
    "\n",
    "Vamos a a partir de un modelo similar al que hemos estado trabajando en la sección anterior, y vamos a comenzar a variar parámetros para entender su impacto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7aYM2yo_470"
   },
   "source": [
    "Vamos a escribir una función que nos permita parametrizar diversos aspectos del model. Entre ellos, la capacidad de generar bloques de capas convolucionales, siendo cada bloque una concatenación de capas de este tipo seguidas de una capa de tipo Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMp076RiGYeD"
   },
   "outputs": [],
   "source": [
    "def gen_conv_model(n_blocks=1, n_conv_layers_per_block=1, filter_size=5, n_filters=32, optimizer=\"adam\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  emb = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "  emb_drop = layers.Dropout(0.5)(emb)\n",
    "\n",
    "  for _ in range(n_blocks):\n",
    "    for _ in range(n_conv_layers_per_block):\n",
    "      fmaps = layers.Conv1D(\n",
    "          n_filters,\n",
    "          filter_size,\n",
    "          padding=\"valid\",\n",
    "          activation=\"relu\",\n",
    "          strides=1)(emb_drop)\n",
    "    pfmaps = layers.MaxPooling1D(pool_size=2)(fmaps)\n",
    "\n",
    "\n",
    "  gpfmaps = layers.GlobalMaxPooling1D()(pfmaps)\n",
    "\n",
    "  # We add a vanilla hidden layer:\n",
    "  classifier = layers.Dense(32, activation=\"relu\")(gpfmaps)\n",
    "  classifier_drop = layers.Dropout(0.5)(classifier)\n",
    "\n",
    "  # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "  predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(classifier_drop)\n",
    "\n",
    "  ## Model build\n",
    "  conv_model = tf.keras.Model(inputs, predictions)\n",
    "\n",
    "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "  conv_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "  return conv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gsNQhS-AO6o"
   },
   "source": [
    "Vamos a generar un par de callbacks que pueden ayudarnos en los entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sJL1tN0Giqs"
   },
   "outputs": [],
   "source": [
    "learning_rate_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=0.001\n",
    ")\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=5,\n",
    "    min_delta=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRw71lGcATpI"
   },
   "source": [
    "A la hora de entrenar los modelos, vamos a fijar el número de epocs por el momento, eliminando el callback de `early_stopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StGm52PkFmaH"
   },
   "outputs": [],
   "source": [
    "def train_conv_model(model):\n",
    "  return model.fit(\n",
    "      seq_train_ds,\n",
    "      validation_data=seq_test_ds,\n",
    "      epochs=10,\n",
    "      callbacks=[\n",
    "          learning_rate_cb,\n",
    "          #early_stopping_cb\n",
    "      ],\n",
    "      verbose = 0\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZNvuWMivC8v"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnXkNTJ3Mv4S"
   },
   "source": [
    "### Tamaño del filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqC-5smFAheW"
   },
   "source": [
    "En primer lugar vamos a intentar ver el impacto que tiene el tamaño del filtro en el modelo. Comenzaremos con una única capa `Conv1D`, y variaremos su tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuP6UWbJNt_r"
   },
   "outputs": [],
   "source": [
    "tests = range(1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OmWA-HF3Nzc4",
    "outputId": "53bec9f1-af6f-4ae5-cf2d-52a4b4564e8d"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, filter_size in enumerate(tests):\n",
    "  tf.keras.backend.clear_session()\n",
    "  print('Testing filter size:', filter_size)\n",
    "  model = gen_conv_model(filter_size=filter_size)\n",
    "  hist = train_conv_model(model)\n",
    "  acc = max(hist.history['val_accuracy'])\n",
    "  epoch = max(hist.epoch)\n",
    "  results.append(\n",
    "      {\n",
    "        'test_number': i,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'filter_size': filter_size\n",
    "      }\n",
    "  )\n",
    "  del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "-uo7aITcRjTc",
    "outputId": "924dbcda-9342-4f91-f23d-40ed42d60da7"
   },
   "outputs": [],
   "source": [
    "filter_size_results_df = pd.DataFrame(results)\n",
    "ax1 = sns.lineplot(data=filter_size_results_df, x=\"filter_size\", y=\"acc\", color=\"blue\")\n",
    "ax2 = plt.twinx()\n",
    "ax2 = sns.lineplot(data=filter_size_results_df, x=\"filter_size\", y=\"epoch\", color=\"red\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w78rytcMBqsV"
   },
   "source": [
    "Al parecer por las pruebas, existe una tendencia a mejorar el rendimiento del modelo al aumentar el tamaño del filtro. Es cierto que hay un pico con filtros de tamaño 2, y habría que realizar más pruebas para ver si es un caso aislado, o para ese tamaño de filtro existe un aumento real de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcVZui86NHMb"
   },
   "source": [
    "### Número de filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzf-PCspCdUK"
   },
   "source": [
    "A continuación vamos a variar el número de filtros, y medir su impacto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGURrrYUO_nG"
   },
   "outputs": [],
   "source": [
    "tests = range(1,250,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HE8jlNSWO-jG",
    "outputId": "d68fcd07-a39f-4558-d2b6-a76e9a46d41b"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, n_filters in enumerate(tests):\n",
    "  tf.keras.backend.clear_session()\n",
    "  print('Testing n_filters:', n_filters)\n",
    "  model = gen_conv_model(filter_size=5, n_filters=n_filters)\n",
    "  hist = train_conv_model(model)\n",
    "  acc = max(hist.history['val_accuracy'])\n",
    "  epoch = max(hist.epoch)\n",
    "  results.append(\n",
    "      {\n",
    "        'test_number': i,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'n_filters': n_filters\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "FPijxCgdgMP6",
    "outputId": "62ecb159-b38f-45b6-a19e-9d7c12453d96"
   },
   "outputs": [],
   "source": [
    "n_filters_results_df = pd.DataFrame(results)\n",
    "ax1 = sns.lineplot(data=n_filters_results_df, x=\"n_filters\", color=\"blue\", y=\"acc\")\n",
    "ax2 = plt.twinx()\n",
    "ax2 = sns.lineplot(data=n_filters_results_df, x=\"n_filters\", color=\"red\", y=\"epoch\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t56yObg2Cn-H"
   },
   "source": [
    "A partir de 30-40 filtros vemos cómo el rendimiento no aumenta al aumentar el número de filtros. Para el problema que estamos resolviendo, y un dataset contenido, parece que una mayor cantidad de filtros no aporta gran cosa. Previamente ya habíamos \"sospechado\" que para 32 filtros ya podían existir filtros poco relevantes para la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56GqiYSmPbwb"
   },
   "source": [
    "### Número de capas convolucionales en un bloque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgyUng5CC0hB"
   },
   "source": [
    "En este caso vamos a generar un único bloque de capas convolucionales, y exploraremos qué ocurre cuando éste bloque aumenta en profundidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OexITrkDPq7V"
   },
   "outputs": [],
   "source": [
    "tests = range(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLrdi1lnPgS8",
    "outputId": "e55c439d-81f8-46ff-8897-52965cf811d8"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, n_conv_layers_per_block in enumerate(tests):\n",
    "  tf.keras.backend.clear_session()\n",
    "  print('Testing n_conv_layers_per_block:', n_conv_layers_per_block)\n",
    "  model = gen_conv_model(\n",
    "      filter_size=15,\n",
    "      n_filters=32,\n",
    "      n_conv_layers_per_block=n_conv_layers_per_block\n",
    "  )\n",
    "  hist = train_conv_model(model)\n",
    "  acc = max(hist.history['val_accuracy'])\n",
    "  epoch = max(hist.epoch)\n",
    "  results.append(\n",
    "      {\n",
    "        'test_number': i,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'n_conv_layers_per_block': n_conv_layers_per_block\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "odaoDwtPgSNf",
    "outputId": "73667c5f-171c-445c-dbd1-fea68abe8382"
   },
   "outputs": [],
   "source": [
    "n_conv_layers_per_block_results_df = pd.DataFrame(results)\n",
    "ax1 = sns.lineplot(data=n_conv_layers_per_block_results_df, x=\"n_conv_layers_per_block\", color=\"blue\", y=\"acc\")\n",
    "ax2 = plt.twinx()\n",
    "ax2 = sns.lineplot(data=n_conv_layers_per_block_results_df, x=\"n_conv_layers_per_block\", color=\"red\", y=\"epoch\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAmQOI-uDsAR"
   },
   "source": [
    "Aunque el rango de pruebas no ha sido demaisado grande, se aprecia una clara caída con el aumento en profundidad del bloque. Dado que hemos fijado el número de epochs, es posible que con mayor profundidad se requieran entrenamientos más largos. \n",
    "\n",
    "Para descartar esto, vamos a realizar un entrenamiento aislado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dg6b6CDabJgw"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NpRBkpDbNA-"
   },
   "outputs": [],
   "source": [
    "model = gen_conv_model(\n",
    "    filter_size=15,\n",
    "    n_filters=32,\n",
    "    n_conv_layers_per_block=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cam1ukZbYap",
    "outputId": "729fbe19-858d-4399-a9f1-d314e05fd8b8"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    seq_train_ds,\n",
    "    validation_data=seq_test_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        learning_rate_cb,\n",
    "        early_stopping_cb\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAU-OD6GFmnN"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rv28R81EUEZ"
   },
   "source": [
    "Tras este entrenamiento se aprecia el fenómeno de overfitting claro, y un detrimento de la precisión al avanzar en el entrenamiento, por lo tanto descartamos nuestra hipótesis anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gPebe2nNLXe"
   },
   "source": [
    "### Número de bloques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztFFVhjDFxPk"
   },
   "source": [
    "Veamos ahora qué ocurre cuando aumentamos el número de bloques que introducimos en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVgRIZm3QGLk"
   },
   "outputs": [],
   "source": [
    "tests = range(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVuqe42oQIHO",
    "outputId": "211527dd-599c-41e1-c18c-4d366d9ef1f8"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, n_blocks in enumerate(tests):\n",
    "  tf.keras.backend.clear_session()\n",
    "  print('Testing n_blocks:', n_blocks)\n",
    "  model = gen_conv_model(\n",
    "      filter_size=15,\n",
    "      n_filters=32,\n",
    "      n_conv_layers_per_block=1,\n",
    "      n_blocks=n_blocks\n",
    "  )\n",
    "  hist = train_conv_model(model)\n",
    "  acc = max(hist.history['val_accuracy'])\n",
    "  epoch = max(hist.epoch)\n",
    "  results.append(\n",
    "      {\n",
    "        'test_number': i,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'n_blocks': n_blocks\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "_yPd9a0TgYOl",
    "outputId": "87e5068d-fba1-4ece-c46b-0daf0d0aaa91"
   },
   "outputs": [],
   "source": [
    "n_blocks_results_df = pd.DataFrame(results)\n",
    "ax1 = sns.lineplot(data=n_blocks_results_df, x=\"n_blocks\", y=\"acc\", color=\"blue\")\n",
    "ax2 = plt.twinx()\n",
    "ax2 = sns.lineplot(data=n_blocks_results_df, x=\"n_blocks\", y=\"epoch\", color=\"red\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rakh_Nt6GLi8"
   },
   "source": [
    "Entre 1 y 4 bloques hemos obtenido un rendimiento creciente. Sin embargo, a partir de ese número de bloques los resultados de la progresión en el número de filtros se vuelve errática. Dada la complejidad de modelos con esa profundidad, es difícil dar una explicación en este momento, y requeriría una serie de pruebas específicas.\n",
    "\n",
    "Por el momento tomamos 4 como un buen número de bloques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjH55brWNS_7"
   },
   "source": [
    "### Embeddings pre-entrenados\n",
    "\n",
    "Hasta el momento todas nuestras pruebas se han llevado a cabo entrenando embeddings desde cero para este problema. \n",
    "\n",
    "Vamos a ver el impacto que tiene utilizar embeddings pre-entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiu343fFHeW2"
   },
   "source": [
    "En primer lugar entrenamos un modelo sencillo utilzando embeddings entrenados desde cero, como hemos hecho hasta ahora, y después replicaremos el problema con una vectorización pre-ajustada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65sVZ7WboO1V",
    "outputId": "48e51466-5ded-463f-af5c-a7d6cf769671"
   },
   "outputs": [],
   "source": [
    "conv_model = create_simple_conv_model(vocab_size=VOCAB_SIZE + 1, num_labels=2)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaX1bAGNoOoz",
    "outputId": "43fd93a6-e24b-4ecc-cc39-376567709111"
   },
   "outputs": [],
   "source": [
    "history = conv_model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "OGV6f-l51BSS",
    "outputId": "a4c26416-9a5b-47cf-ad50-7ce067b34593"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ5d4_s7H33b"
   },
   "source": [
    "En este punto hemos obtenido una precisión cercana al 84% para este problema.\n",
    "\n",
    "Vamos a utilizar un embedding de GloVe con 50 dimensiones (es el más cercano a las 64 que hemos estado usando), congelaremos la capa de Embedding para que no sufra cambios con el entrenamiento. Esto nos dará un modelo con menos parámetros, y por tanto más estable de cara al entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TbrvyhhoOd2"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHGd1LPbIYVL"
   },
   "source": [
    "Cargamos los vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfTOdXNrpYzd",
    "outputId": "3b67171b-a59e-41d0-8e2d-79504360b360"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    \"glove.6B.50d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTYOeywFqWRv"
   },
   "outputs": [],
   "source": [
    "voc = seq_vectorize_layer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0ijnkYlIdEm"
   },
   "source": [
    "Generamos ahora la matriz de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3K91EbXpiG_",
    "outputId": "f32e7ae6-b66d-4c4e-afc4-d3b0b4e89da7"
   },
   "outputs": [],
   "source": [
    "num_tokens = VOCAB_SIZE + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aU7gbW6RrjIi",
    "outputId": "6379d0e8-aacf-4cee-88b0-19853782a5a5"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83zQ773nIg2T"
   },
   "source": [
    "Y creamos un modelo como el que utilizamos en el entrenamiento anterior, pero cargando los pesos del nuvo embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oc5YDtnJqy39"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(\n",
    "        VOCAB_SIZE + 2,\n",
    "        50,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False\n",
    "    ),\n",
    "    layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "model.compile(\n",
    "  loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wjsg6W2brqYk",
    "outputId": "b2e3fa76-c099-416f-bef3-275928a191f6"
   },
   "outputs": [],
   "source": [
    "history = model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "wEUhAhrX1KVQ",
    "outputId": "b508d5d2-8f04-4891-8097-a0a93fed98e9"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRq63Sn8IqUT"
   },
   "source": [
    "Podemos ver como los resultados obtenidos son notablemente inferiores al modelo en el que se entrenó esta capa como parte del modelo (un 10% de caída en precisión).\n",
    "\n",
    "Esto en parte se debe a que esta capa no ha sido entrenada con el fin de resolver este problema, por lo que nuestro siguiente paso será replicar este entrenamiento, pero sin congelar la capa de embedding, de manera que puedan modificarse sus valores durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhVknOVmrqPs"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(\n",
    "        VOCAB_SIZE + 2,\n",
    "        50,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "model.compile(\n",
    "  loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elbSvw8mqkgG",
    "outputId": "f1aa1b86-626a-448a-8469-b9f3b6538437"
   },
   "outputs": [],
   "source": [
    "history = model.fit(seq_train_ds, validation_data=seq_test_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "veSZJBhk1NY6",
    "outputId": "a669d9a7-3379-402a-f432-b810a7891eee"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDJ-GNKmJPwC"
   },
   "source": [
    "Podemos observar cómo hay una mejora de la precisión (sobrepasamos el 80%), pero sin llegar a los resultado previos.\n",
    "\n",
    "Entrenar un modelo con valores reentrenados es un proceso complejo, pues para modificar el embedding preentrenado deberíamos usar optimizadores con learning rates reducidos, mientras que el resto del modelo requiere un tratamiento más agresivo.\n",
    "\n",
    "Sin duda podríamos llegar a la misma precisión que utilizando un embedding entrenado desde cero, pero por el momento parece que introduciría complejidad al entrenamiento sin obtener grandes beneficios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3rOHiawZKkd"
   },
   "source": [
    "### Cambios en el tratamiento del texto\n",
    "\n",
    "Hasta el momento no hemos prestado atención al tipo de procesado que sufría el texto, así que vamos ha ver cómo impacta en los resultados.\n",
    "\n",
    "Como en la práctica anterior se profundizó en este tema, no vamos a dar excesivos detalles. Probaremos los tres niveles de pre procsado del texto que vimos al inicio de esta misma práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7zJO12LxZRjd",
    "outputId": "a55de31e-7439-46ab-c4ef-4d8d8bc6e25e"
   },
   "outputs": [],
   "source": [
    "tests = ['basic', 'lemma', 'stem']\n",
    "\n",
    "for t in tests:\n",
    "  print(f\"Training with {t} processing level...\")\n",
    "  tf.keras.backend.clear_session()\n",
    "  raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (train_df[f'{t}_review'], train_df['label'].values)).batch(BATCH_SIZE)\n",
    "  raw_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (test_df[f'{t}_review'], test_df['label'].values)).batch(BATCH_SIZE)\n",
    "\n",
    "  seq_vectorize_layer = TextVectorization(\n",
    "      max_tokens=VOCAB_SIZE,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "  train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "  seq_vectorize_layer.adapt(train_text)\n",
    "\n",
    "  seq_train_ds = raw_train_ds.map(seq_vectorize_text)\n",
    "  seq_test_ds = raw_test_ds.map(seq_vectorize_text)\n",
    "\n",
    "  seq_train_ds = configure_dataset(seq_train_ds)\n",
    "  seq_test_ds = configure_dataset(seq_test_ds)\n",
    "\n",
    "  conv_model = gen_conv_model(\n",
    "      filter_size=15,\n",
    "      n_filters=32,\n",
    "      n_conv_layers_per_block=1,\n",
    "      n_blocks=4\n",
    "  )\n",
    "\n",
    "  history = conv_model.fit(\n",
    "      seq_train_ds, validation_data=seq_test_ds, epochs=5, verbose=0)\n",
    "\n",
    "  plot_history(history)\n",
    "  print(\"Test acc:\", history.history['val_accuracy'][-1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-jHPqA3QQO8"
   },
   "source": [
    "Podemos ver cómo la lematización y el stemming aportan un extra respecto al pre-procesado básico, probablemente debido a la unificación de términos con carga semántica casi similar, que implica una mayor condensación de la información.\n",
    "\n",
    "Entre la lematización y el stemming, los resultado son muy similares, estando la lematización levemente mejor posicionada en nuestras pruebas. Sin embargo, esa diferencia podría no ser significativa dada la simplicidad de las pruebas.\n",
    "\n",
    "Por otra parte, el stemming es limitante con frecuencia a la hora de utilizar embeddings pre entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZBiaeO3ZP9s"
   },
   "source": [
    "### Algoritmos de optimización\n",
    "\n",
    "Llevaremos a cabo una comparativa en cuanto a los algoritmos de optimización, no tanto para encontrar el mejor de ellos (aquí se requeriría un trabajo importante de meta optimización de parámetros de cada uno de ellos), si no para entender si hay grandes diferencias entre unos y otros con este tipo de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzoTzQDsSOA2"
   },
   "source": [
    "Dados los resultados del apartado anterior, vamos a partir del dataset con lematización aplicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coS7rRcU4daG"
   },
   "outputs": [],
   "source": [
    "raw_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "      (train_df[f'lemma_review'], train_df['label'].values)).batch(BATCH_SIZE)\n",
    "raw_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df[f'lemma_review'], test_df['label'].values)).batch(BATCH_SIZE)\n",
    "\n",
    "seq_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "seq_vectorize_layer.adapt(train_text)\n",
    "\n",
    "seq_train_ds = raw_train_ds.map(seq_vectorize_text)\n",
    "seq_test_ds = raw_test_ds.map(seq_vectorize_text)\n",
    "\n",
    "seq_train_ds = configure_dataset(seq_train_ds)\n",
    "seq_test_ds = configure_dataset(seq_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GXR8JUE-58Sj",
    "outputId": "56853bed-beb2-44d9-a674-8f19a959860f"
   },
   "outputs": [],
   "source": [
    "for optimizer in [\"adam\", \"adagrad\", \"nadam\", \"ftrl\", \"nadam\"]:\n",
    "  print(\"Optimizer:\", optimizer)\n",
    "  tf.keras.backend.clear_session()\n",
    "  ## Model build\n",
    "  conv_model = gen_conv_model(\n",
    "      filter_size=15,\n",
    "      n_filters=32,\n",
    "      n_conv_layers_per_block=1,\n",
    "      n_blocks=4\n",
    "  )\n",
    "\n",
    "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "  conv_model.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "  history = conv_model.fit(\n",
    "        seq_train_ds,\n",
    "        validation_data=seq_test_ds,\n",
    "        epochs=20,\n",
    "        callbacks=[\n",
    "            learning_rate_cb,\n",
    "            early_stopping_cb\n",
    "        ],\n",
    "        verbose=0\n",
    "  )\n",
    "\n",
    "  print(\"Test acc:\", history.history['val_accuracy'][-1], \"\\n\")\n",
    "  plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6Kaq6fLSgPH"
   },
   "source": [
    "Como vemos, hay grandes diferencias entre algoritmos de optimización, pudiendo lastrar mucho el aprendizaje del modelo si la elección no es correcta. \n",
    "\n",
    "Entre aquellos que ofrecen un mejor rendimiento, como se ha comentado, faltaría realizar un trabajo de meta optimización de parámetros específicos de esos algoritmos para optener los mejores resultados. Consideramos que esto está fuera del alcance de esta práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqkrSjgrPcDO"
   },
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hIpY6oSTEGm"
   },
   "source": [
    "En esta práctica hemos repasado modelos que ofrecen un paso más en capacidad de abstracción y complejidad sobre lo que se vió en la práctica anterior. Previamente trabajábamos con la presencia o no de palabras en un texto, y trabajamos con la presencia o no de ciertos patrones, no palabras aisladas. Esto es un aspecto fundamental para el tratamiento de textos, dado que no es lo mismo \"it is a good movie\" que \"it is **not** a good movie\".\n",
    "\n",
    "Sin embargo, este avance en capacidades del modelo va ligado a una mayor complejidad, y require un mayor conocimiento de los modelos para poder entender qué está ocurriendo. Sin embargo, como hemos visto que la explicabilidad de estos modelos, una vez que se entienden sus mecánicas, es posible.\n",
    "\n",
    "De entre todos los factores existentes a la hora de desarrollar un modelo, entender el propio modelo es fundamental para poder mejorarlo. Por lo tanto considero que esta práctica es tremendamente útil."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
