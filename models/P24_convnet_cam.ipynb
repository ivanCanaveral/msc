{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100baf5c-993a-4371-8eb5-acfe610aefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe2207-c98e-42ac-aa41-3d75dfa6c646",
   "metadata": {},
   "source": [
    "RamprasaathR.Selvarajuetal.,arXiv(2017),https://arxiv.org/abs/1610.02391."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e8c28-6704-4266-9b3a-acc2f7759708",
   "metadata": {},
   "source": [
    "Grad-CAM consists of taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the channel. Intuitively, one way to understand this trick is to imagine that you’re weighting a spatial map of “how intensely the input image acti- vates different channels” by “how important each channel is with regard to the class,” resulting in a spatial map of “how intensely the input image activates the class.”\n",
    "\n",
    "This time we will need model's top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b14995-65b1-4b11-9c2d-000dd5a84f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.xception.Xception(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12831d39-b5b1-45e6-8af4-6c34108db931",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = tf.keras.utils.get_file(\n",
    "    fname=\"elephant.jpg\",\n",
    "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=target_size)\n",
    "    array = tf.keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = tf.keras.applications.xception.preprocess_input(array)\n",
    "    return array\n",
    "\n",
    "img_array = get_img_array(img_path, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a476a1b-7653-486c-ac91-b95dbfbe7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(img_array)\n",
    "print(tf.keras.applications.xception.decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc85ed9-9638-4515-800c-a293dc90ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f1872-48d2-4d1e-8a89-66108116b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"block14_sepconv2_act\"\n",
    "classifier_layer_names = [\n",
    "    \"avg_pool\",\n",
    "    \"predictions\",\n",
    "]\n",
    "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d97562-ba18-4d99-a053-338c3ce776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "x = classifier_input\n",
    "for layer_name in classifier_layer_names:\n",
    "    x = model.get_layer(layer_name)(x)\n",
    "classifier_model = tf.keras.Model(classifier_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e4b00-6004-4ba7-8f2a-d72f49679ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "    tape.watch(last_conv_layer_output)\n",
    "    preds = classifier_model(last_conv_layer_output)\n",
    "    top_pred_index = tf.argmax(preds[0])\n",
    "    top_class_channel = preds[0, top_pred_index]\n",
    "\n",
    "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8a3fa-9ee9-46ad-a031-82ba800f6107",
   "metadata": {},
   "source": [
    "$$ f_{i^{th}class}: \\mathbb{R}^{10 \\times 10 \\times 2048} \\rightarrow \\mathbb{R}^{2048} \\rightarrow \\mathbb{R} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7102f3-7be7-43b1-8d28-b1e59c4e016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads.shape, top_class_channel.shape, last_conv_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4dfc58-be8d-43fb-960a-f0761d2889ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(grads, axis=(0, 1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2bc99-f460-4364-b389-81674cce0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
    "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "for i in range(pooled_grads.shape[-1]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef936e9-c41e-4436-832f-e7be87b6261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_output.shape, pooled_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc301a-a1f7-42e9-bd63-7bd31ebd7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(pooled_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435bb4e-bc6e-481a-8174-38a022664e60",
   "metadata": {},
   "source": [
    "## Heatmap from all filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106edfe-eea1-4550-ae1a-ba297fa153c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.mean(last_conv_layer_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47e38d-fcfc-4930-bc9b-dede993e0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dcaa36-179c-4490-9f5f-464a93271c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 1e-10)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70936829-fc30-4537-bfb2-82ad9db8e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "img = tf.keras.utils.load_img(img_path)\n",
    "img = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "jet = cm.get_cmap(\"jet\")\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
    "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "jet_heatmap = tf.keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "superimposed_img = jet_heatmap * 0.4 + img\n",
    "superimposed_img = tf.keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "save_path = \"elephant_cam.jpg\"\n",
    "superimposed_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059be7b7-5f24-470e-99c2-a70b7b7eb316",
   "metadata": {},
   "source": [
    "## Heatmaps from each filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fe467-5355-41bc-bf22-33bfa93feb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_heatmap(heatmap, img_path, index, act):\n",
    "    folder = 'filter_cams'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    img = tf.keras.utils.load_img(img_path)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = tf.keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    superimposed_img = jet_heatmap * 0.4 + img\n",
    "    superimposed_img = tf.keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "    save_path = f\"{folder}/elephant_cam_str_{index:04d}_{act:08f}.jpg\"\n",
    "    superimposed_img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ba02d-a1ff-4303-a7bd-36da1218b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_heatmap(heatmap):\n",
    "    heatmap = np.maximum(heatmap, 1e-10)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    #plt.matshow(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279943e-4a1b-458f-a277-69462abf04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(last_conv_layer_output.shape[-1]):\n",
    "    heatmap = last_conv_layer_output[:, :, i]\n",
    "    avg_activation = np.mean(heatmap)\n",
    "    if avg_activation > 1e-05:\n",
    "        heatmap = normalize_heatmap(heatmap)\n",
    "        save_heatmap(heatmap, img_path, i, avg_activation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
