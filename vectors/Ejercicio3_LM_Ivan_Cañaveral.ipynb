{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkVmKkNoWzAf"
   },
   "source": [
    "# Ejercicio práctico - Tema 3: Modelo de Lenguaje\n",
    "\n",
    "**Alumno**: Iván Cañaveral Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAyU6GsEN7lW"
   },
   "source": [
    "## Modelo de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA3v0gQ8fKA0"
   },
   "source": [
    "Para dar respuesta a lso distintos apartados de este ejercicio práctico vamos a definir una clase cuyos métodos irán utilizándose en los distintos apartados.\n",
    "\n",
    "Esta clase toma como parámetros:\n",
    "\n",
    "* El corpus de entrenamiento\n",
    "* Una variable binaria que indica si se aplica o no Laplace Smoothing.\n",
    "\n",
    "Los métodos más relevantes que implementa son:\n",
    "\n",
    "* **word_prob**: calcula la probabilidad condicionada de la palabra `w_n`, dado que aparece `w_(n-1)`.\n",
    "* **next_most_suitable_word**: calcula la siguiente palabra más probable dada una secuencia.\n",
    "* **sentence_prob**: calcula la probabilidad de una frase.\n",
    "* **amount_of_information**: calcula la cantidad de información de una frase.\n",
    "* **perplexity**: calcula la perplejidad de una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztUwdTJIXb9e"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9Z1YamYcv7W"
   },
   "outputs": [],
   "source": [
    "class BigramModel():\n",
    "  \"\"\"\n",
    "  Class for languaje models based on bigrams.\n",
    "  A n-gram model can be developed using this class,\n",
    "  just changing previous_words by a list or string\n",
    "  of previous words.\n",
    "  \"\"\"\n",
    "  def __init__(self, corpus, laplace_smoothing=False):\n",
    "    self.corpus = corpus\n",
    "    self.laplace_smoothing = laplace_smoothing\n",
    "    self.tokenizer = lambda text: re.split(\"\\\\s+\", text)\n",
    "    self.text = ' '.join(corpus)\n",
    "    self.words = list(\n",
    "        set(self.tokenizer(self.text.lower()))\n",
    "    )\n",
    "    self.vocab_size = len(self.words)\n",
    "    self.bigrams = list(\n",
    "        set(\n",
    "            [' '.join(pair) for pair in product(self.words, self.words) \\\n",
    "             if ' '.join(pair) in self.text.lower()]\n",
    "            )\n",
    "        )\n",
    "    self.bigram_cv = CountVectorizer(\n",
    "        ngram_range=(2,2),\n",
    "        vocabulary=self.bigrams,\n",
    "        tokenizer=self.tokenizer\n",
    "    )\n",
    "    self.word_cv = CountVectorizer(\n",
    "        vocabulary=self.words,\n",
    "        tokenizer=self.tokenizer\n",
    "    )\n",
    "    self.word_count = self._get_word_count()\n",
    "    self.bigram_count = self._get_bigram_count()\n",
    "\n",
    "  def _get_word_count(self):\n",
    "    x = self.word_cv.fit_transform(self.corpus)\n",
    "    words = self.word_cv.get_feature_names_out()\n",
    "    counts = x.todense().sum(axis=0).tolist()[0]\n",
    "    return {word: count for word, count in zip(words, counts)}\n",
    "\n",
    "  def _get_bigram_count(self):\n",
    "    x = self.bigram_cv.fit_transform(self.corpus)\n",
    "    bigrams = self.bigram_cv.get_feature_names_out()\n",
    "    counts = x.todense().sum(axis=0).tolist()[0]\n",
    "    return {word: count for word, count in zip(bigrams, counts)}\n",
    "\n",
    "  def word_prob(self, word, previous_word):\n",
    "    \"\"\"\n",
    "    MLE : bigram_count / all_bigrams_cointaining_previous_word\n",
    "    all_bigrams_cointaining_previous_word = previous_word_count\n",
    "\n",
    "    if laplace_smoothing:\n",
    "      bigram count += 1\n",
    "      previous_word_count += vocab size\n",
    "    \"\"\"\n",
    "    word, previous_word = word.lower(), previous_word.lower()\n",
    "    bigram = previous_word + ' ' + word\n",
    "    previous_word_count = self.word_count.get(previous_word, 0)\n",
    "    bigram_count = self.bigram_count.get(bigram, 0)\n",
    "    if self.laplace_smoothing:\n",
    "      previous_word_count += self.vocab_size - 1\n",
    "      bigram_count += 1\n",
    "    return bigram_count / previous_word_count\n",
    "  \n",
    "  def _get_next_word_probs(self, previous_word):\n",
    "    return {word: self.word_prob(word, previous_word) for word in self.words}\n",
    "\n",
    "  def next_most_suitable_word(self, sentence):\n",
    "    \"\"\"\n",
    "    It gets the most suitable word after a sentence.\n",
    "    In case of two values with the same prob, it gets\n",
    "    one of them.\n",
    "    \"\"\"\n",
    "    previous_word = sentence.split()[-1]\n",
    "    next_word_probs = self._get_next_word_probs(previous_word)\n",
    "    return sorted(\n",
    "        next_word_probs.items(), key=lambda item: item[1], reverse=True)[0]\n",
    "\n",
    "  def sentence_prob(self, sentence):\n",
    "    \"\"\"\n",
    "    Prob as the composition of independent events approximation.\n",
    "    p(w_i|w_{1...(i-1)}) ~= p(w_i|w_{1-i})\n",
    "    then\n",
    "    p(w_0, w_1, ..., w_i) = p(w_0) * p(w_1|w_0) * ··· * p(w_i|p(w_(i-1))\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().split()\n",
    "    pairs = sliding_window_view(sentence, 2)\n",
    "    probs = [self.word_prob(word, prev_word) for prev_word, word in pairs]\n",
    "    return np.prod(probs)\n",
    "  \n",
    "  def amount_of_information(self, sentence):\n",
    "    prob = self.sentence_prob(sentence)\n",
    "    return -np.log(prob), prob\n",
    "\n",
    "  def perplexity(self, sentence):\n",
    "    n = len(self.tokenizer(sentence))\n",
    "    sentence_prob = self.sentence_prob(sentence)\n",
    "    return np.power(sentence_prob, 1/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWp0YWv-N-_c"
   },
   "source": [
    "## Corpus\n",
    "\n",
    "A continuación definimos el conjunto de entrenamiento dado para la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvd-gvCMOH3U"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "  \"<s> I am John </s>\",\n",
    "  \"<s> John I am </s>\",\n",
    "  \"<s> John I like </s>\",\n",
    "  \"<s> John I do like </s>\",\n",
    "  \"<s> do I like John </s>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj8ZvFByOp1J"
   },
   "source": [
    "## Modelo sin suavizado\n",
    "\n",
    "En este apartado vamos a ajustar un modelo con el corpus proporcionado, y vamos a resolver ls distintas secciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5fjlJ26jCDM"
   },
   "outputs": [],
   "source": [
    "bm = BigramModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvYC7eCYP2P5"
   },
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dSn1zOSPF7w"
   },
   "source": [
    "Una vez ajustado, vamos a comprobar que el procesado del corpus de prueba ha funcionado correctamente, revisando los conteos de las distintas palabras y los bigramas de los textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phfM7_4PjFtO",
    "outputId": "41110fcf-8452-4551-fffc-e367bc060560"
   },
   "outputs": [],
   "source": [
    "bm._get_word_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mb14NTjCjrMR",
    "outputId": "198f7707-0828-4c41-da30-01c4f74cccaf"
   },
   "outputs": [],
   "source": [
    "bm._get_bigram_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzfRhMaUPnmR"
   },
   "source": [
    "Antes de empezar con los ejercicios, vamos a hacer un par de cálculos sencillos de probabilidades para comprobar que las probabilidades se calculan adecuadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QE01TvLXwOv",
    "outputId": "cb2e38ed-def0-4c50-8a4f-ff9393cc64ff"
   },
   "outputs": [],
   "source": [
    "bm.word_prob('I', '<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FxLjAGCkzcA",
    "outputId": "93544ede-b6d5-417a-b10f-3443f9aa822a"
   },
   "outputs": [],
   "source": [
    "bm.word_prob('I', 'John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1HRwQrpoRH6",
    "outputId": "52a8afaa-5cf4-4a5c-b097-209757a61108"
   },
   "outputs": [],
   "source": [
    "bm.sentence_prob(\"I am John\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjLETF9hP9lG"
   },
   "source": [
    "### Ejercicios\n",
    "\n",
    "Vamos a resolver los ejercicios relativos al modelo sin suavizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPa3ZJOVrJvN"
   },
   "source": [
    "#### 1. ¿Cuál será la palabra siguiente más probable predicha por el modelo para las siguientes secuencias de palabras?\n",
    "* (1) `<s> John ...`\n",
    "* (2) `<s> John I do ...`\n",
    "* (3) `<s> John I am John ...`\n",
    "* (4) `<s> do I like ...:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUyQCNJdp_J9"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"<s> John\",\n",
    "  \"<s> John I do\",\n",
    "  \"<s> John I am John\",\n",
    "  \"<s> do I like\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MhqzSpLWrqdC",
    "outputId": "ab5c5e72-8c60-4b9b-da5e-4bf517f036a8"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "  word, prob = bm.next_most_suitable_word(sentence)\n",
    "  print(f\"Sentence: {sentence} ... \\t Next word: {word}. \\t Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFjvScYvuqfP"
   },
   "source": [
    "#### 2. ¿Cuál de las siguientes frases tiene menor cantidad de información según la teoría de Shannon?\n",
    "* (5) `<s> John I do I like </s>`\n",
    "* (6) `<s> John I am </s>`\n",
    "* (7) `<s> I do like John I am </s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fDWQYK7vEKQ"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"<s> John I do I like </s>\",\n",
    "  \"<s> John I am </s>\",\n",
    "  \"<s> I do like John I am </s>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJ0Eg5BCsMnC",
    "outputId": "bd0f5ee2-fcaf-424c-aca0-57b77d95b4c7"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "  info, prob = bm.amount_of_information(sentence)\n",
    "  print(f\"Sentences: {sentence}. \\t Probability: {prob}. \\t Amount of information: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzoYCPm4Q6PT"
   },
   "source": [
    "La frase que tiene asociada una menor cantidad de información es la segunda, que se corresponde con la de mayor probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yhw9OdIcxYXs"
   },
   "source": [
    "#### Considerando de nuevo el mismo corpus de entrenamiento y el mismo modelo de bigramas, calcule la perplejidad de la siguiente frase:\n",
    "* `<s> I do like John`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1L6tgs-zEXc"
   },
   "outputs": [],
   "source": [
    "sentence = \"<s> I do like John\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6jb59Zey8yf",
    "outputId": "149d8139-69b9-47b3-eead-429218973569"
   },
   "outputs": [],
   "source": [
    "bm.perplexity(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am4JLWRfOwTi"
   },
   "source": [
    "## Modelo con suavizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhgvBqIW111v"
   },
   "source": [
    "Tomando de nuevo los mismos datos de entrenamiento, pero esta vez utilizando\n",
    "un modelo de bigramas con suavizado de Laplace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBszkHu_12vd"
   },
   "outputs": [],
   "source": [
    "bm_smoothing = BigramModel(corpus, laplace_smoothing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDYswef413j_"
   },
   "source": [
    "#### Obtenga las probabilidades que asignaría este modelo de bigramas para:\n",
    "* `P(do|<s>)`\n",
    "* `P(do|John)`\n",
    "* `P(John|<s>)`\n",
    "* `P(John|do)`\n",
    "* `P(I|John)`\n",
    "* `P(I|do)`\n",
    "* `P(like|I)`\n",
    "\n",
    "**Nota**: para cada palabra wn-1, contamos un bigrama adicional para cada posible continuación wn. En consecuencia, habrá que tener en cuenta, tanto las palabras, como el símbolo de fin de frase (</s>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tXEmPo08WNs"
   },
   "source": [
    "Respecto a la nota, estamos tratanto `</s>` como un componente más del vocabulario, por lo que al sumar `V` al numerador, ya se tiene en cuenta. Sin embargo, dado que `w_n` nunca será `<s>` cuando se disponga `w_(n-1)`, en nuestro cado debemos añadir `V - 1` al denominador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncy-Eu_Z6G8r",
    "outputId": "61d4ada6-e3b9-4be6-ba8f-c192bb6f277f"
   },
   "outputs": [],
   "source": [
    "bm_smoothing.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQPc14rO8QIP"
   },
   "outputs": [],
   "source": [
    "pairs = [\n",
    "  (\"do|<s>\"),\n",
    "  (\"do|John\"),\n",
    "  (\"John|<s>\"),\n",
    "  (\"John|do\"),\n",
    "  (\"I|John\"),\n",
    "  (\"I|do\"),\n",
    "  (\"like|I\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ql2j8aK9s7o",
    "outputId": "e42d9957-fd42-42ba-f5f3-44651d5ab5e4"
   },
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "  word, prev_word = pair.split('|')\n",
    "  prob = bm.word_prob(word, prev_word)\n",
    "  prob_smoothing = bm_smoothing.word_prob(word, prev_word)\n",
    "  print(\"Bigram:\", prev_word, word)\n",
    "  print(f\"Without regularization: P({pair}) = {prob}\")\n",
    "  print(f\"With regularization: P({pair}) = {prob_smoothing} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwl6oKgaJ6YO"
   },
   "source": [
    "#### Calcule las probabilidades de las siguientes secuencias de palabras según\n",
    "este modelo de lenguaje:\n",
    "* (8) `<s> do John I like`\n",
    "* (9) `<s> John do I like`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlXV_5uq-Hp3"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"<s> do John I like\",\n",
    "    \"<s> John do I like\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlJpwPRKKNdR",
    "outputId": "b05297e5-de7a-48a2-e2d5-2f76b4845793"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "  prob = bm.sentence_prob(sentence)\n",
    "  prob_smoothing = bm_smoothing.sentence_prob(sentence)\n",
    "  print(\"Sentence:\", sentence)\n",
    "  print(f\"Without regularization: P({sentence}) = {prob}\")\n",
    "  print(f\"With regularization: P({sentence}) = {prob_smoothing} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dCrMfkrRoBk"
   },
   "source": [
    "Para ambos, la probabilidad sin regularización es 0 porque ambos contienen un bigrama de probabilidad cero dado el corpus de entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RErdmWLhSIQB",
    "outputId": "a70d1b8a-627a-49bb-dbf0-5c1e0d4482ba"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "  print('\\n Sentence', sentence)\n",
    "  sentence = bm.tokenizer(sentence.lower())\n",
    "  pairs = sliding_window_view(sentence, 2)\n",
    "  probs = [bm.word_prob(word, prev_word) for prev_word, word in pairs]\n",
    "  for pair, prob in zip(pairs, probs):\n",
    "    print(\"  \", pair, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUpWhCo0TP-r"
   },
   "source": [
    "Que ambos tengan la misma probabilidad tras al aplicar suavizado es simplemente casualidad, como podemos observar a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp2DHRZ-TaN2",
    "outputId": "e349d529-5370-41f8-a29a-6a1e4a17f2b1"
   },
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "  print('\\n Sentence', sentence)\n",
    "  sentence = bm_smoothing.tokenizer(sentence.lower())\n",
    "  pairs = sliding_window_view(sentence, 2)\n",
    "  probs = [bm_smoothing.word_prob(word, prev_word) for prev_word, word in pairs]\n",
    "  for pair, prob in zip(pairs, probs):\n",
    "    print(\"  \", pair, prob)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
